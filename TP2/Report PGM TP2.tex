\documentclass[11pt,a4paper]{article}

\usepackage{etex}
\usepackage[applemac]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[francais]{babel}
\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage{stmaryrd}
\usepackage{float}
\pagestyle{plain}
\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}

\usepackage{tikz}
\usetikzlibrary{arrows}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

	\title{Probabilistic Graphical Models : Assignment 2}
	\author{Mathurin \textsc{Massias} \and ClŽment \textsc{Nicolle}}
	\date{\today}

\begin{document}
	
	\maketitle

\section{Distributions factorizing in a graph}

\hspace*{-6mm}\textbf{(a)}
\\i. This is a covered edge, for example :
\\
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

\node[main node] (1) {};
\node[main node] (2) [below of=1] {i};
\node[main node] (3) [right of=2] {j};
\node[main node] (4) [below of=2] {};

\path[every node/.style={font=\sffamily\small}]
	(1) edge node {} (2)
	edge node {} (3)
	(2) edge node {} (3)
	(4) edge node  {} (2)
	edge node {} (3);

\end{tikzpicture}
\end{center}
%
The parents of the node $j$ are $\left\lbrace  i + parents\:of\:i \right\rbrace$. We now reverse $i \rightarrow j$. Let show that $\mathcal{L}(G) = \mathcal{L}(G')$
\\
\\
If $p(x) \in \mathcal{L(G)}$ we have :
\\$p(x) = \mathop{\Pi}\limits_{k=1}^N p(x_k | x_{\pi_k}) = \mathop{\Pi}\limits_{k \not\in \left\lbrace i,j \right\rbrace} p(x_k | x_{\pi_k}) \times p(x_i | x_{\pi_i}) \times p(x_j | x_{\pi_j}) $
\\
But, with $\pi_j = \pi_i \cup \left\lbrace i \right\rbrace$, we have :
\\
$p(x_i | x_{\pi_i}) \times p(x_j | x_{\pi_j}) = p(x_i | x_{\pi_i}) \times p(x_j | x_{\pi_i}, x_i) = p(x_i | x_{\pi_i}) \times p(x_j | x_{\pi_i})$
\\ \-\hspace{1cm} because $x_i$ depends on $x_{\pi_i}$
\\And if we reverse  $i \rightarrow j$, $x_{\pi_i}$ becomes $x_{\pi_j}$ and the value of the product above will be the same.
\\
\\
So $\mathcal{L}(G) = \mathcal{L}(G')$
\\
\\
ii. Let $G$ be a directed tree without v-structures and $G'$ the undirected tree with the same edges (the symmetrized graph). Let show that $\mathcal{L}(G) = \mathcal{L}(G')$
\\
\\
As there is no v-structures in $G$, it means that the cliques of $G'$ are only composed of couples : $C= \left\lbrace (x_i, x_{\pi_i}) \right\rbrace$ (each $x_i$ has a single parent).
\\Thus we have $p(x) = \mathop{\Pi}\limits_{c=1}^N p(x_c | x_{\pi_c}) = \frac{1}{Z} \mathop{\Pi}\limits_{c \in C} \psi_c (x_c) $ with $\psi_c (x_c) \: \alpha \: p(x_c | x_{\pi_c})$
\\
\\
So $\mathcal{L}(G) = \mathcal{L}(G')$
\\
\\
\textbf{(b)}
\\According to (a)ii., if the number of nodes is 1 or 2 we have no v-structures so $\mathcal{L}(G) = \mathcal{L}(G')$.
For 3 nodes, let oblige to have a v-structure in the directed graph. If we choose an undirected graph where the three nodes are linked, we will compulsorily have a v-structure, as below :

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]

\node[main node] (1) {1};
\node[main node] (2) [below of=1] {2};
\node[main node] (3) [right of=2] {3};

\path[every node/.style={font=\sffamily\small}]
(1) edge node {} (2)
	edge node {} (3)
(2) edge node {} (3);

\end{tikzpicture}
\end{center}

The symmetrized graph is composed of only one maximal clique. And we have $p(x) = p(x_1) \times p(x_2 | x_1) \times p(x_3 | x_1, x_2)$, which can be written as $\psi(x_1, x_2, x_3)$

For 4 nodes, if we consider directed acyclic graph, we will also have a v-structure, in node 4 below for instance : 

\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
	thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
	
	\node[main node] (1) {1};
	\node[main node] (2) [below of=1] {2};
	\node[main node] (3) [right of=1] {3};
	\node[main node] (4) [right of=2] {4};
	
	\path[every node/.style={font=\sffamily\small}]
	(1) edge node {} (2)
	edge node {} (3)
	(2) edge node {} (4)
	(3) edge node {} (4);
	
	\end{tikzpicture}
\end{center}

We now have $p(x) = p(x_1) \times p(x_2 | x_1) \times p(x_3 | x_1) \times p(x_4 | x_2, x_3)$. But, as there is a v-structure and the maximal cliques in the undirected graph are only composed of two elements, we cannot write $p(x)$ as $\frac{1}{Z} ( \psi_1 (x_1, x_2) \times \psi_2 (x_1,x_2) \times \psi_3 (x_2, x_3) \times \psi_4 (x_3, x_4) )$


\section{d-separation}
%
%
\hspace*{-6mm}\textbf{(a)}
\\If $G = (V,E)$, the moralized graph is $G_M = (V,E')$ with $E' = E \cup \left\lbrace (i,j) / \exists k \in V \: s.t. \: i,j \in \pi_k \right\rbrace$
\\Let's take a chain $(v_1,...,v_N)$ from A to B in $G$. It is also a chain in $G_M$. As S separates A and B in $G_M$, we know that this chain goes through S, which means it exists i such that $v_i \in S$.
\\We can consider without loss of generality that this i is unique (if the chain goes through consecutive vertices in S we group them in one, and if it goes through several disjoint vertices we can cut it in different subchains with only one vertice in S).
\\From here, we have two cases :
\\- $(v_{i-1},v_i,v_{i+1})$ is not a v-structure in $G$. Then we have that $v_i \in S$ blocks the chain.
\\- $(v_{i-1},v_i,v_{i+1})$ is a v-structure in $G$.
\\Then $v_{i-1}$ and $v_{i+1}$ are linked in $G_M$ because they are both parent of $v_i$. So the chain $(v_1,...,v_{i-1},v{i+1},...,v_N)$ goes from A to B in $G_M$. Then, this chains goes through S in $G_M$, which is absurd because we made the assumptions that $v_i$ was the unique vertice in S from the chain. \\So $(v_{i-1},v_i,v_{i+1})$ cannot be a v-structure.
\\
\\To sum up, every chains from A to B go through S and there is no v-structure on such vertices. So A and B are d-separated by S in $G$.
\\
\\\textbf{(b)}
\\No, this is not true. Here is a counterexample :

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
thick,main node/.style={circle,draw,font=\sffamily\Large\bfseries}]
	
\node[main node] (1) {};
\node[main node] (2) [below right of=1] {S};
\node[main node] (3) [above right of=1] {A};
\node[main node] (4) [right of=3] {};
\node[main node] (5) [below right of=4] {T};
\node[main node] (6) [above right of=5] {};
\node[main node] (7) [right of=6] {B};
	
\path[every node/.style={font=\sffamily\small}]
(1) edge node {} (2)
edge node {} (3)
(3) edge node {} (4)
(4) edge node {} (5)
(6) edge node {} (5)
edge node {} (7);
	
	\end{tikzpicture}
\end{center}
S separates A and B as the only chain that goes from A to B contains a v-structure on T which is not in S.
\\T separates A and S as the only chain that goes from A to B contains a v-structure on a vertice which is not in T.
\\But T does not separates A and B, because the only chain that goes from A to B contains a v-structure on T.
\\
\\\textbf{(c)}
\\ We know that if A and B are d-separated by S, then $X_A \indep X_B | X_C$.
\\- $X_{\left\lbrace 1,2 \right\rbrace} \indep X_4 | X_3$ : True.
\\3 separates $\left\lbrace 1,2 \right\rbrace$ and 4 because the chains from $\left\lbrace 1,2 \right\rbrace$ to 4 have a v-structure on 8 which is not an ancestor of 3.
\\
\\- $X_{\left\lbrace 1,2 \right\rbrace} \indep X_4 | X_5$ : False, because 5 is a descendant of 8 where we have a v-structure.
\\
\\$X_1 \indep X_6 | X_{\left\lbrace 2,4,7 \right\rbrace}$ : True
\\All chains from 1 to 6 go through 7 with no v-structure.

\section{Implementation of K Means and Gaussian mixture}
%
%
\hspace*{-6mm}\textbf{(a)}
Running our algorithm with a different seed each time, we see that it can converge to different results, some of them quite bad (seed=44 gives a distorsion of 1500, whereas usually distorsion is around 1100).
\\We see the sensibility of KMeans with respect to initialization : the algorithm can get stuck at a local minimum. However most of the time the alglorithm converge to an acceptable solution (distortion around 1100).
\\We also notice that, even in the "good case" (i.e. low distorsion), the algorithm puts the upper right extremity of the "long cluster"  (blue in figure 2) into another cluster (green in figure 2), to whom they don't seem to belong, because there's a gap between them and the other points of the green cluster. This problem will no longer be present with EM.
\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.4]{kmeans_pourri.png}
\noindent\includegraphics[scale=0.4]{distortion_seed44.png}
\caption{Clusters and centroids obtained by K-Means for seed = 44, and distortion measure at each iteration (starting arbitrarily to 0)}
\end{figure}

\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.4]{kmeans_good.png}
\noindent\includegraphics[scale=0.4]{distortion_seed45.png}
\caption{Clusters and centroids obtained by K-Means for seed = 45, and distortion measure at each iteration (starting arbitrarily to 0)}
\end{figure}
%
\hspace*{-6mm}\textbf{Example of detailed results} : for seed = 45, distortion at local minimum is 1105.8 and the four centroids we obtain are (in order : red, blue, green, yellow): 
$\begin{pmatrix} 
-2.24 & -3.66 & 3.48 & 3.80\\
4.24 & -4.07 & -2.70 & 5.10
\end{pmatrix}$
%
\\[5mm]\textbf{(b)}
\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.5]{circles.png}
\caption{Data set and 4 gaussians with their level sets (circles), from 10\% to 90\%}
\end{figure}
As we forced the variances to be proportional to $I_2$, we get that level sets are circles, because if $\Sigma  = \sigma^2 I_2, \sigma \neq 0$, we have 
%
$$(x-\mu)^T \Sigma^{-1} (x- \mu) = \frac{1}{\sigma^2} \Vert x - \mu \Vert ^2$$
%
\\Instead of updating the respective variances with the empirical covariance matrices of each cluster, for each cluster $i$ at each iteration we chose to take the covariance matrix equal to $V_i I_2$ where $V_i$ is the variance of the distances of the points belonging to this cluster, to the mean of the cluster. $V_i =$Var($ \Vert x - \mu_i \Vert$) for $x$ belonging to cluster i.
%
\\[5mm]The circles do not seem to fit the data very well, except for the yellow and (in a lesser way) red clusters. And even for these, some points are in the blue cluster when they should be in the yellow or red one instead.
%
\\[5mm]
\textbf{Detailed results for Figure 3}
\\- Posterior probabilities (order: yellow, blue, red, green) :
$\begin{pmatrix} 
0.224 & 0.296 & 0.214 & 0.266\\
\end{pmatrix}$
%
\\- Means (same order) :
$\begin{pmatrix} 
-2.29 & -3.38 & 3.82 & 3.67 \\
4.31 & -3.77 & -3.50 & 4.34 
\end{pmatrix}$
%
\\- Covariances matrices (same order, giving only the diagonal term) :
$\begin{pmatrix} 
2.21 & 8.7 & 1.35 & 5.23\\
\end{pmatrix}$
%
%
\\[5mm]\textbf{(c)}
\begin{figure}[H]
\centering
\noindent\includegraphics[scale=0.5]{ellipsis.png}
\caption{Data set and 4 gaussians with their level sets (ellipsis), from 10\% to 90\%}
\end{figure}
Since no assumption is made on the 4 variances, the level sets are now ellipsis, whose axis are not necessarily parallel to $(Ox)$ and $(Oy)$ respectively. Had we imposed that the variances should be diagonal, we would have obtained ellipsis with such axis.
\\We see a much better fit, especially for the long, diagonal cluster (blue). The "covariance proportional to identity" hypothesis seems to have been reasonable only for one cluster, the yellow one.
%
\\[5mm]
\textbf{Detailed results for Figure 4}
\\\textit{Caution, colors have changed ! Refer to Figure 4}
\\- Posterior probabilities (order on Figure 4: yellow, blue, red, green) :
$\begin{pmatrix} 
0.248 & 0.304 & 0.206 & 0.242\\
\end{pmatrix}$
%
\\- Means (same order) :
$\begin{pmatrix} 
-2.09 & -3.11 & 3.79 & 3.98 \\
4.19 & -3.58 & -3.65 & 4.37 
\end{pmatrix}$
%
\\- Covariances matrices (same order) :
\\
$\begin{pmatrix} 
2.75 & 0.21 \\ 
0.21 & 2.65
\end{pmatrix}$
 ("proportional to identity" hypothesis was valid for this cluster (yellow on Figure 4))
%
\\
$\begin{pmatrix} 
5.93 & 5.76 \\ 
5.76 & 5.92
\end{pmatrix}$
(the long, blue cluster has a covariance matrix very far from being proportional to identity)
%
\\
$\begin{pmatrix} 
0.86 & 0.04 \\ 
0.04 & 2.03
\end{pmatrix}$
("almost" diagonal : axis are nearly vertical and horizontal, but the vertical axis is bigger than the horizontal one)
%
\\
$\begin{pmatrix} 
0.22 & 0.16 \\ 
0.16 & 9.05
\end{pmatrix}$
(same, axis are nearly horizontal and vertical, but the vertical axis is much bigger)
%
\\[5mm]
\textbf{(d)} 
It is important to notice that the datasets have the same size, otherwise this comparison would make no sense.
\\The loglikelihood is higher for ellipsis on both datasets : relaxing the "covariance proportional to identity" constraint allows the model to better fit the data.
\\
\begin{tabular}{|l|c|c|}
  \hline
  & Training set& Test set \\
  \hline
  Circles & -2697 & -2650 \\
  Ellipsis & -2344 & -2423 \\
  \hline
\end{tabular}
\\
\\We see  no obvious overfitting. However, it is worth noticing that the likelihood increases from Training to Test for circles, whereas it decreases for ellipsis : the general covariance matrices model has more degrees of freedom, therefore it is more likely to overfit.
\end{document}